{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ecle\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from sklearn.metrics import log_loss\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-07 00:40:18,197 Reading data from /home/jmf/.flair/datasets/ud_english\n",
      "2019-11-07 00:40:18,198 Train: /home/jmf/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2019-11-07 00:40:18,199 Test: /home/jmf/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "2019-11-07 00:40:18,200 Dev: /home/jmf/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "2019-11-07 00:40:28,437 Reading data from /home/jmf/.flair/datasets/wikiner_english\n",
      "2019-11-07 00:40:28,438 Train: /home/jmf/.flair/datasets/wikiner_english/aij-wikiner-en-wp3.train\n",
      "2019-11-07 00:40:28,439 Dev: None\n",
      "2019-11-07 00:40:28,439 Test: None\n",
      "2019-11-07 00:41:24,626 Reading data from /home/jmf/.flair/datasets/wnut_17\n",
      "2019-11-07 00:41:24,628 Train: /home/jmf/.flair/datasets/wnut_17/wnut17train.conll\n",
      "2019-11-07 00:41:24,629 Dev: /home/jmf/.flair/datasets/wnut_17/emerging.dev.conll\n",
      "2019-11-07 00:41:24,630 Test: /home/jmf/.flair/datasets/wnut_17/emerging.test.annotated\n",
      "2019-11-07 00:41:26,312 Reading data from /home/jmf/.flair/datasets/imdb\n",
      "2019-11-07 00:41:26,313 Train: /home/jmf/.flair/datasets/imdb/train.txt\n",
      "2019-11-07 00:41:26,315 Dev: None\n",
      "2019-11-07 00:41:26,317 Test: /home/jmf/.flair/datasets/imdb/test.txt\n",
      "2019-11-07 00:41:27,206 Reading data from /home/jmf/.flair/datasets/conll_2000\n",
      "2019-11-07 00:41:27,207 Train: /home/jmf/.flair/datasets/conll_2000/train.txt\n",
      "2019-11-07 00:41:27,208 Dev: None\n",
      "2019-11-07 00:41:27,209 Test: /home/jmf/.flair/datasets/conll_2000/test.txt\n"
     ]
    }
   ],
   "source": [
    "import flair\n",
    "import flair.datasets\n",
    "from flair.data import MultiCorpus\n",
    "wiki_ner = flair.datasets.WIKINER_ENGLISH()\n",
    "wiki_nut = flair.datasets.WNUT_17()\n",
    "imdb = flair.datasets.IMDB()\n",
    "conll = flair.datasets.CONLL_2000()\n",
    "multi = MultiCorpus([conll, wiki_ner, wiki_nut, imdb])\n",
    "sents = [i.to_plain_string() for i in multi.train+multi.dev+multi.test]\n",
    "# with open(\"data/wikisents3.txt\", 'r') as f:\n",
    "#     sents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = [i.to_plain_string() for i in multi.train+multi.dev+multi.test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204790/204790 [00:59<00:00, 3462.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ecle_inp_3:0\", shape=(?, 128), dtype=float32) []\n",
      "[<tf.Tensor 'pretraining_final_dense_3/Softmax:0' shape=(?, 10000) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "prep = ecle.Preprocessor(num_stopwords=20)\n",
    "\n",
    "\n",
    "\n",
    "np.random.shuffle(sents)\n",
    "# sents = sents[:10000]\n",
    "num_test = 4000\n",
    "sents_train, sents_test = sents[:-num_test], sents[-num_test:]\n",
    "prep.fit(sents_train, progbar=True)\n",
    "from ecle.pretraining import MaskedLanguagePretrainer\n",
    "mlp = MaskedLanguagePretrainer(preprocessor=prep, base_args={\"preprocessor\":prep})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ecle_inp (InputLayer)           (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ecle_embedding (Embedding)      (None, 128, 32)      2816        ecle_inp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c1 (Conv1D)                (None, 128, 256)     24832       ecle_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 256)     0           ecle_c1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c2 (Conv1D)                (None, 128, 256)     196864      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c3 (Conv1D)                (None, 128, 256)     196864      ecle_c2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 256)     0           ecle_c3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ecle_res_conn (Add)             (None, 128, 256)     0           ecle_c1[0][0]                    \n",
      "                                                                 ecle_c3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c4 (Conv1D)                (None, 128, 256)     196864      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecle_res_conn_2 (Add)           (None, 128, 256)     0           ecle_res_conn[0][0]              \n",
      "                                                                 ecle_c4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c5 (Conv1D)                (None, 128, 1024)    787456      ecle_res_conn_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "pretraining_core_c1 (Conv1D)    (None, 128, 128)     393344      ecle_c5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128, 128)     0           pretraining_core_c1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_5 (Permute)             (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_7 (Permute)             (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pretraining_core_att_0 (Dense)  (None, 128, 128)     16512       permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pretraining_core_att_1 (Dense)  (None, 128, 128)     16512       permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pretraining_core_att_2 (Dense)  (None, 128, 128)     16512       permute_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pretraining_core_att_3 (Dense)  (None, 128, 128)     16512       permute_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 128, 128)     0           pretraining_core_att_0[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 128, 128)     0           pretraining_core_att_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "permute_6 (Permute)             (None, 128, 128)     0           pretraining_core_att_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "permute_8 (Permute)             (None, 128, 128)     0           pretraining_core_att_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "                                                                 permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "                                                                 permute_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "                                                                 permute_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 512)     0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "                                                                 multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pretraining_core_c2 (Conv1D)    (None, 128, 128)     196736      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pretraining_core_c3 (Conv1D)    (None, 128, 128)     49280       pretraining_core_c2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           pretraining_core_c3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pretraining_final_dense (Dense) (None, 10000)        1290000     global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,401,104\n",
      "Trainable params: 3,401,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.model.compile('sgd', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bad strings\n",
    "sents_train_clean = []\n",
    "sents_test_clean = []\n",
    "for example in sents_train:\n",
    "    sp = [tok.strip() for tok in example.split(\" \") if tok.strip() != \"\"]\n",
    "    if len(sp) < 2:\n",
    "        continue\n",
    "    # normalize to see what we can replace\n",
    "    normed = [prep.scrub(tok) for tok in sp]\n",
    "    # see which tokens are in the vocabulary\n",
    "    replaceable_tokens = [\n",
    "        (t, i) for i, t in enumerate(normed) if t in prep.token_dict\n",
    "    ]\n",
    "    if len(replaceable_tokens) > 0:\n",
    "        sents_train_clean.append(example)\n",
    "for example in sents_test:\n",
    "    sp = [tok.strip() for tok in example.split(\" \") if tok.strip() != \"\"]\n",
    "    if len(sp) < 2:\n",
    "        continue\n",
    "    # normalize to see what we can replace\n",
    "    normed = [prep.scrub(tok) for tok in sp]\n",
    "    # see which tokens are in the vocabulary\n",
    "    replaceable_tokens = [\n",
    "        (t, i) for i, t in enumerate(normed) if t in prep.token_dict\n",
    "    ]\n",
    "    if len(replaceable_tokens) > 0:\n",
    "        sents_test_clean.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the tfhub reps\n",
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "# embed = hub.Module(module_url)\n",
    "\n",
    "# input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "# embedding_layer = embed(input_placeholder)\n",
    "# with tf.Session() as session:\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "#     session.run(tf.tables_initializer())\n",
    "#     y_train = np.zeros((len(sents_train), 512))\n",
    "#     y_test = np.zeros((len(sents_test), 512))\n",
    "#     for n, d in enumerate(sents_train):\n",
    "#         y_train[n] = session.run(embedding_layer,\n",
    "#                                  feed_dict={input_placeholder: [d]})\n",
    "#     for n, d in enumerate(sents_test):\n",
    "#         y_test = session.run(embedding_layer,\n",
    "#                              feed_dict={input_placeholder: [d]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jmf/miniconda3/envs/recipe/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jmf/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# from keras.layers import Input, SeparableConv1D, Conv1D, MaxPooling1D, Lambda, Multiply\n",
    "# from keras.layers import Dropout, Dense, Embedding, Dot, Concatenate, Reshape, Add, Average\n",
    "# from keras.layers import GlobalMaxPooling1D, GlobalMaxPooling2D, Permute, RepeatVector, Flatten\n",
    "# from keras.layers import BatchNormalization\n",
    "# from keras.layers.wrappers import TimeDistributed\n",
    "# from keras.regularizers import l1_l2\n",
    "# from keras.optimizers import Adam, RMSprop, SGD\n",
    "# from keras.models import Model, load_model\n",
    "# from keras.initializers import TruncatedNormal\n",
    "# from keras.constraints import MinMaxNorm\n",
    "# import keras.backend as K\n",
    "# import tensorflow as tf\n",
    "\n",
    "# ecle_base = ecle.ECLEModelBase(preprocessor=prep)\n",
    "\n",
    "# new_conv = Conv1D(128, 3, padding=\"same\", name=\"words\", activation=\"tanh\")(\n",
    "#     ecle_base.embedder.output\n",
    "# )\n",
    "# new_conv = Dropout(0.25)(new_conv)\n",
    "# # add attention heads\n",
    "# n_attention_heads = 4\n",
    "# new_conv_shape = int(new_conv.shape[1])\n",
    "# att_heads = []\n",
    "# for head_num in range(n_attention_heads):\n",
    "#     att = Permute((2, 1))(new_conv)\n",
    "#     att = Dense(new_conv_shape, activation=\"softmax\", name=f\"att_{head_num}\")(att)\n",
    "#     att = Permute((2, 1))(att)\n",
    "#     att_out = Multiply()([new_conv, att])\n",
    "#     att_heads.append(att_out)\n",
    "# cat = Concatenate()(att_heads)\n",
    "\n",
    "# final_conv = Conv1D(512, 3, padding=\"same\", activation=\"relu\")(cat)\n",
    "\n",
    "# # pool and predict\n",
    "# gmp = GlobalMaxPooling1D()(final_conv)\n",
    "# d = Dense(prep.vocab_size, activation='softmax')(gmp)\n",
    "\n",
    "# model = Model(ecle_base.embedder.input, d)\n",
    "# language_model_prerained = Model(ecle_base.embedder.input, final_conv)\n",
    "# model.compile(SGD(), 'categorical_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.9184 - val_loss: 7.8833\n",
      "Epoch 2/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.9016 - val_loss: 7.8127\n",
      "Epoch 3/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8913 - val_loss: 7.8775\n",
      "Epoch 4/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8977 - val_loss: 7.8204\n",
      "Epoch 5/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.9002 - val_loss: 7.9391\n",
      "Epoch 6/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8837 - val_loss: 7.9026\n",
      "Epoch 7/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8864 - val_loss: 7.7321\n",
      "Epoch 8/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8836 - val_loss: 7.9070\n",
      "Epoch 9/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8841 - val_loss: 7.8587\n",
      "Epoch 10/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8809 - val_loss: 7.8796\n",
      "Epoch 11/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8869 - val_loss: 7.9004\n",
      "Epoch 12/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8793 - val_loss: 7.7403\n",
      "Epoch 13/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8799 - val_loss: 7.9268\n",
      "Epoch 14/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8664 - val_loss: 8.0152\n",
      "Epoch 15/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8708 - val_loss: 7.8933\n",
      "Epoch 16/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8752 - val_loss: 7.8277\n",
      "Epoch 17/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8705 - val_loss: 7.7659\n",
      "Epoch 18/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8768 - val_loss: 7.9117\n",
      "Epoch 19/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8772 - val_loss: 7.9042\n",
      "Epoch 20/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8818 - val_loss: 7.8305\n",
      "Epoch 21/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8755 - val_loss: 7.8989\n",
      "Epoch 22/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8783 - val_loss: 7.7992\n",
      "Epoch 23/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8670 - val_loss: 7.9001\n",
      "Epoch 24/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8734 - val_loss: 7.6952\n",
      "Epoch 25/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8815 - val_loss: 7.8570\n",
      "Epoch 26/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8685 - val_loss: 7.8097\n",
      "Epoch 27/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8684 - val_loss: 7.8879\n",
      "Epoch 28/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8779 - val_loss: 7.8256\n",
      "Epoch 29/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8600 - val_loss: 7.8140\n",
      "Epoch 30/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8703 - val_loss: 7.8737\n",
      "Epoch 31/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8794 - val_loss: 7.8553\n",
      "Epoch 32/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8657 - val_loss: 7.9024\n",
      "Epoch 33/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8716 - val_loss: 7.8992\n",
      "Epoch 34/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8778 - val_loss: 7.7203\n",
      "Epoch 35/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8746 - val_loss: 7.8242\n",
      "Epoch 36/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8661 - val_loss: 7.8435\n",
      "Epoch 37/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8714 - val_loss: 7.8717\n",
      "Epoch 38/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8684 - val_loss: 7.8235\n",
      "Epoch 39/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8612 - val_loss: 7.8760\n",
      "Epoch 40/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8721 - val_loss: 7.8832\n",
      "Epoch 41/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8729 - val_loss: 7.7818\n",
      "Epoch 42/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8693 - val_loss: 7.8173\n",
      "Epoch 43/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8652 - val_loss: 7.8877\n",
      "Epoch 44/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8677 - val_loss: 7.7188\n",
      "Epoch 45/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8620 - val_loss: 7.8731\n",
      "Epoch 46/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8671 - val_loss: 7.7934\n",
      "Epoch 47/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8616 - val_loss: 7.8500\n",
      "Epoch 48/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8794 - val_loss: 7.8880\n",
      "Epoch 49/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8647 - val_loss: 7.8288\n",
      "Epoch 50/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8665 - val_loss: 7.8581\n",
      "Epoch 51/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8706 - val_loss: 7.9030\n",
      "Epoch 52/1000\n",
      "5000/5000 [==============================] - 184s 37ms/step - loss: 7.8615 - val_loss: 7.8043\n",
      "Epoch 53/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8712 - val_loss: 7.8834\n",
      "Epoch 54/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8689 - val_loss: 7.8494\n",
      "Epoch 55/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8619 - val_loss: 7.9180\n",
      "Epoch 56/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8595 - val_loss: 7.9333\n",
      "Epoch 57/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8706 - val_loss: 7.8285\n",
      "Epoch 58/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8645 - val_loss: 7.8639\n",
      "Epoch 59/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8715 - val_loss: 7.8683\n",
      "Epoch 60/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8700 - val_loss: 7.8758\n",
      "Epoch 61/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8640 - val_loss: 7.8848\n",
      "Epoch 62/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8724 - val_loss: 7.9000\n",
      "Epoch 63/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8615 - val_loss: 7.8339\n",
      "Epoch 64/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8663 - val_loss: 7.8326\n",
      "Epoch 65/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8631 - val_loss: 7.8302\n",
      "Epoch 66/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8681 - val_loss: 7.7261\n",
      "Epoch 67/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8723 - val_loss: 7.9294\n",
      "Epoch 68/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8652 - val_loss: 7.9036\n",
      "Epoch 69/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8563 - val_loss: 7.9601\n",
      "Epoch 70/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8700 - val_loss: 7.8788\n",
      "Epoch 71/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8607 - val_loss: 7.8372\n",
      "Epoch 72/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8716 - val_loss: 7.9324\n",
      "Epoch 73/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8627 - val_loss: 7.8809\n",
      "Epoch 74/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8686 - val_loss: 7.8345\n",
      "Epoch 75/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8573 - val_loss: 7.8183\n",
      "Epoch 76/1000\n",
      "5000/5000 [==============================] - 183s 37ms/step - loss: 7.8590 - val_loss: 7.7820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8635 - val_loss: 7.8064\n",
      "Epoch 78/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8500 - val_loss: 7.8872\n",
      "Epoch 79/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8555 - val_loss: 7.8939\n",
      "Epoch 80/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8744 - val_loss: 7.9115\n",
      "Epoch 81/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8766 - val_loss: 7.9033\n",
      "Epoch 82/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8693 - val_loss: 7.9415\n",
      "Epoch 83/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8791 - val_loss: 7.8600\n",
      "Epoch 84/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8628 - val_loss: 7.8493\n",
      "Epoch 85/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8567 - val_loss: 7.9267\n",
      "Epoch 86/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8650 - val_loss: 7.8608\n",
      "Epoch 87/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8625 - val_loss: 7.8930\n",
      "Epoch 88/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8691 - val_loss: 7.9564\n",
      "Epoch 89/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8553 - val_loss: 7.8507\n",
      "Epoch 90/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8651 - val_loss: 7.9723\n",
      "Epoch 91/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8710 - val_loss: 7.7930\n",
      "Epoch 92/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8617 - val_loss: 7.8447\n",
      "Epoch 93/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8707 - val_loss: 7.8793\n",
      "Epoch 94/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8651 - val_loss: 7.7749\n",
      "Epoch 95/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8602 - val_loss: 7.8952\n",
      "Epoch 96/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8646 - val_loss: 7.7811\n",
      "Epoch 97/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8556 - val_loss: 7.8942\n",
      "Epoch 98/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8641 - val_loss: 7.8401\n",
      "Epoch 99/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8591 - val_loss: 7.8179\n",
      "Epoch 100/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8659 - val_loss: 7.8564\n",
      "Epoch 101/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8738 - val_loss: 7.8111\n",
      "Epoch 102/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8700 - val_loss: 7.8447\n",
      "Epoch 103/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8677 - val_loss: 7.8461\n",
      "Epoch 104/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8610 - val_loss: 7.8345\n",
      "Epoch 105/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8625 - val_loss: 7.8368\n",
      "Epoch 106/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8628 - val_loss: 7.7688\n",
      "Epoch 107/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8591 - val_loss: 7.8401\n",
      "Epoch 108/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8639 - val_loss: 7.8766\n",
      "Epoch 109/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8664 - val_loss: 7.8436\n",
      "Epoch 110/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8652 - val_loss: 7.8924\n",
      "Epoch 111/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8706 - val_loss: 7.8667\n",
      "Epoch 112/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8635 - val_loss: 7.9707\n",
      "Epoch 113/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8679 - val_loss: 7.7778\n",
      "Epoch 114/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8627 - val_loss: 7.9442\n",
      "Epoch 115/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8574 - val_loss: 7.8743\n",
      "Epoch 116/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8571 - val_loss: 7.9094\n",
      "Epoch 117/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8522 - val_loss: 7.8840\n",
      "Epoch 118/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8665 - val_loss: 7.9689\n",
      "Epoch 119/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8598 - val_loss: 7.6813\n",
      "Epoch 120/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8579 - val_loss: 7.8439\n",
      "Epoch 121/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8607 - val_loss: 7.8248\n",
      "Epoch 122/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8607 - val_loss: 7.7893\n",
      "Epoch 123/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8608 - val_loss: 7.8546\n",
      "Epoch 124/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8616 - val_loss: 7.9285\n",
      "Epoch 125/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8616 - val_loss: 7.7957\n",
      "Epoch 126/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8629 - val_loss: 7.7588\n",
      "Epoch 127/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8668 - val_loss: 7.8752\n",
      "Epoch 128/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8587 - val_loss: 7.7916\n",
      "Epoch 129/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8608 - val_loss: 7.8404\n",
      "Epoch 130/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8552 - val_loss: 7.9436\n",
      "Epoch 131/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8576 - val_loss: 7.9642\n",
      "Epoch 132/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8615 - val_loss: 7.8758\n",
      "Epoch 133/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8633 - val_loss: 7.9028\n",
      "Epoch 134/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8665 - val_loss: 7.8478\n",
      "Epoch 135/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8562 - val_loss: 7.9743\n",
      "Epoch 136/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8719 - val_loss: 7.9704\n",
      "Epoch 137/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8593 - val_loss: 7.7952\n",
      "Epoch 138/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8616 - val_loss: 7.7944\n",
      "Epoch 139/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8616 - val_loss: 7.8749\n",
      "Epoch 140/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8652 - val_loss: 7.8576\n",
      "Epoch 141/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8658 - val_loss: 7.9938\n",
      "Epoch 142/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8502 - val_loss: 7.9650\n",
      "Epoch 143/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8491 - val_loss: 7.8771\n",
      "Epoch 144/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8626 - val_loss: 7.8266\n",
      "Epoch 145/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8751 - val_loss: 7.8951\n",
      "Epoch 146/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8655 - val_loss: 7.8968\n",
      "Epoch 147/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8558 - val_loss: 7.8881\n",
      "Epoch 148/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8587 - val_loss: 7.8878\n",
      "Epoch 149/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8532 - val_loss: 7.8742\n",
      "Epoch 150/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8632 - val_loss: 7.7840\n",
      "Epoch 151/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8596 - val_loss: 7.8458\n",
      "Epoch 152/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8678 - val_loss: 7.8861\n",
      "Epoch 153/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8526 - val_loss: 7.9026\n",
      "Epoch 154/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8746 - val_loss: 7.8522\n",
      "Epoch 155/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8653 - val_loss: 7.9665\n",
      "Epoch 156/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8605 - val_loss: 7.7790\n",
      "Epoch 157/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8633 - val_loss: 7.8939\n",
      "Epoch 158/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8651 - val_loss: 7.8495\n",
      "Epoch 159/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8670 - val_loss: 7.7479\n",
      "Epoch 160/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8583 - val_loss: 7.8013\n",
      "Epoch 161/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8729 - val_loss: 7.9301\n",
      "Epoch 162/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8599 - val_loss: 7.8019\n",
      "Epoch 163/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8569 - val_loss: 7.9856\n",
      "Epoch 164/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8625 - val_loss: 7.7413\n",
      "Epoch 165/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8591 - val_loss: 7.8384\n",
      "Epoch 166/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8650 - val_loss: 7.8206\n",
      "Epoch 167/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8582 - val_loss: 7.8944\n",
      "Epoch 168/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8670 - val_loss: 7.8613\n",
      "Epoch 169/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8641 - val_loss: 7.8550\n",
      "Epoch 170/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8542 - val_loss: 7.8911\n",
      "Epoch 171/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8616 - val_loss: 7.8372\n",
      "Epoch 172/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8644 - val_loss: 7.9061\n",
      "Epoch 173/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8579 - val_loss: 7.8738\n",
      "Epoch 174/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8671 - val_loss: 7.8025\n",
      "Epoch 175/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8597 - val_loss: 7.9234\n",
      "Epoch 176/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8600 - val_loss: 7.8913\n",
      "Epoch 177/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8633 - val_loss: 7.9231\n",
      "Epoch 178/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8753 - val_loss: 7.9875\n",
      "Epoch 179/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8625 - val_loss: 7.8155\n",
      "Epoch 180/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8642 - val_loss: 7.9332\n",
      "Epoch 181/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8591 - val_loss: 7.9050\n",
      "Epoch 182/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8590 - val_loss: 7.8495\n",
      "Epoch 183/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8613 - val_loss: 7.9333\n",
      "Epoch 184/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8696 - val_loss: 7.7767\n",
      "Epoch 185/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8492 - val_loss: 7.9230\n",
      "Epoch 186/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8583 - val_loss: 7.7816\n",
      "Epoch 187/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8581 - val_loss: 7.8561\n",
      "Epoch 188/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8667 - val_loss: 7.8627\n",
      "Epoch 189/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8691 - val_loss: 7.7667\n",
      "Epoch 190/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8484 - val_loss: 7.7762\n",
      "Epoch 191/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8550 - val_loss: 7.7824\n",
      "Epoch 192/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8525 - val_loss: 7.8694\n",
      "Epoch 193/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8648 - val_loss: 7.8597\n",
      "Epoch 194/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8500 - val_loss: 7.7459\n",
      "Epoch 195/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8750 - val_loss: 7.8608\n",
      "Epoch 196/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8663 - val_loss: 7.7987\n",
      "Epoch 197/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8614 - val_loss: 7.8651\n",
      "Epoch 198/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8585 - val_loss: 7.9406\n",
      "Epoch 199/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8545 - val_loss: 7.8697\n",
      "Epoch 200/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8670 - val_loss: 7.9061\n",
      "Epoch 201/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8582 - val_loss: 7.9012\n",
      "Epoch 202/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8637 - val_loss: 7.8161\n",
      "Epoch 203/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8492 - val_loss: 7.8656\n",
      "Epoch 204/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8548 - val_loss: 7.7604\n",
      "Epoch 205/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8628 - val_loss: 7.9412\n",
      "Epoch 206/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8419 - val_loss: 7.8612\n",
      "Epoch 207/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8601 - val_loss: 7.9306\n",
      "Epoch 208/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8544 - val_loss: 7.9431\n",
      "Epoch 209/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8729 - val_loss: 7.8173\n",
      "Epoch 210/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8615 - val_loss: 7.9197\n",
      "Epoch 211/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8548 - val_loss: 7.8532\n",
      "Epoch 212/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8647 - val_loss: 7.8927\n",
      "Epoch 213/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8596 - val_loss: 7.8056\n",
      "Epoch 214/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8595 - val_loss: 7.7916\n",
      "Epoch 215/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8658 - val_loss: 7.9316\n",
      "Epoch 216/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8554 - val_loss: 7.8362\n",
      "Epoch 217/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8654 - val_loss: 7.8537\n",
      "Epoch 218/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8570 - val_loss: 7.8810\n",
      "Epoch 219/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8574 - val_loss: 7.8406\n",
      "Epoch 220/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8574 - val_loss: 7.8924\n",
      "Epoch 221/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8645 - val_loss: 7.7186\n",
      "Epoch 222/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8656 - val_loss: 7.9661\n",
      "Epoch 223/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8637 - val_loss: 7.8485\n",
      "Epoch 224/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8569 - val_loss: 7.8549\n",
      "Epoch 225/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8754 - val_loss: 7.9865\n",
      "Epoch 226/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8483 - val_loss: 7.8982\n",
      "Epoch 227/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8582 - val_loss: 7.8980\n",
      "Epoch 228/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8643 - val_loss: 7.9033\n",
      "Epoch 229/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8504 - val_loss: 7.7692\n",
      "Epoch 230/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8715 - val_loss: 7.8581\n",
      "Epoch 231/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8632 - val_loss: 7.8107\n",
      "Epoch 232/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8588 - val_loss: 7.7843\n",
      "Epoch 233/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8512 - val_loss: 7.7988\n",
      "Epoch 234/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8682 - val_loss: 7.7170\n",
      "Epoch 235/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8673 - val_loss: 7.9025\n",
      "Epoch 236/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8634 - val_loss: 7.9698\n",
      "Epoch 237/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8553 - val_loss: 7.8520\n",
      "Epoch 238/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8576 - val_loss: 7.9094\n",
      "Epoch 239/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8674 - val_loss: 7.8455\n",
      "Epoch 240/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8729 - val_loss: 7.8675\n",
      "Epoch 241/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8669 - val_loss: 7.7828\n",
      "Epoch 242/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8555 - val_loss: 7.9324\n",
      "Epoch 243/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8634 - val_loss: 7.8706\n",
      "Epoch 244/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8712 - val_loss: 7.7445\n",
      "Epoch 245/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8556 - val_loss: 7.9031\n",
      "Epoch 246/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8613 - val_loss: 7.9684\n",
      "Epoch 247/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8720 - val_loss: 7.9161\n",
      "Epoch 248/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8500 - val_loss: 7.8557\n",
      "Epoch 249/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8548 - val_loss: 7.8349\n",
      "Epoch 250/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8673 - val_loss: 7.8636\n",
      "Epoch 251/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8608 - val_loss: 7.9712\n",
      "Epoch 252/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8744 - val_loss: 7.8408\n",
      "Epoch 253/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8596 - val_loss: 7.8106\n",
      "Epoch 254/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8539 - val_loss: 7.7356\n",
      "Epoch 255/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8688 - val_loss: 7.7902\n",
      "Epoch 256/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8560 - val_loss: 7.9250\n",
      "Epoch 257/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8539 - val_loss: 7.7201\n",
      "Epoch 258/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8655 - val_loss: 7.8817\n",
      "Epoch 259/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8678 - val_loss: 7.8584\n",
      "Epoch 260/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8541 - val_loss: 7.9382\n",
      "Epoch 261/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8696 - val_loss: 7.7804\n",
      "Epoch 262/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8633 - val_loss: 7.6814\n",
      "Epoch 263/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8559 - val_loss: 7.9113\n",
      "Epoch 264/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8676 - val_loss: 7.7793\n",
      "Epoch 265/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8614 - val_loss: 7.8847\n",
      "Epoch 266/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8640 - val_loss: 7.7315\n",
      "Epoch 267/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8663 - val_loss: 7.8898\n",
      "Epoch 268/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8655 - val_loss: 7.9197\n",
      "Epoch 269/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8671 - val_loss: 7.7789\n",
      "Epoch 270/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8596 - val_loss: 7.8392\n",
      "Epoch 271/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8503 - val_loss: 7.8634\n",
      "Epoch 272/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8678 - val_loss: 7.9239\n",
      "Epoch 273/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8623 - val_loss: 7.8750\n",
      "Epoch 274/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8603 - val_loss: 7.8431\n",
      "Epoch 275/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8550 - val_loss: 7.8644\n",
      "Epoch 276/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8650 - val_loss: 7.7857\n",
      "Epoch 277/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8485 - val_loss: 7.8274\n",
      "Epoch 278/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8531 - val_loss: 7.8397\n",
      "Epoch 279/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8598 - val_loss: 7.9151\n",
      "Epoch 280/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8645 - val_loss: 7.9391\n",
      "Epoch 281/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8624 - val_loss: 7.9173\n",
      "Epoch 282/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8612 - val_loss: 7.9385\n",
      "Epoch 283/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8627 - val_loss: 7.8698\n",
      "Epoch 284/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8673 - val_loss: 7.9023\n",
      "Epoch 285/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8630 - val_loss: 7.9155\n",
      "Epoch 286/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8607 - val_loss: 7.7713\n",
      "Epoch 287/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8688 - val_loss: 7.9721\n",
      "Epoch 288/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8632 - val_loss: 7.8210\n",
      "Epoch 289/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8303 - val_loss: 7.8355\n",
      "Epoch 290/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8592 - val_loss: 7.9334\n",
      "Epoch 291/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8501 - val_loss: 7.8822\n",
      "Epoch 292/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8553 - val_loss: 7.8949\n",
      "Epoch 293/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8602 - val_loss: 7.9089\n",
      "Epoch 294/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8642 - val_loss: 7.6660\n",
      "Epoch 295/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8593 - val_loss: 7.9107\n",
      "Epoch 296/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8634 - val_loss: 7.8958\n",
      "Epoch 297/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8583 - val_loss: 7.8985\n",
      "Epoch 298/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8641 - val_loss: 7.8494\n",
      "Epoch 299/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8583 - val_loss: 7.7887\n",
      "Epoch 300/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8553 - val_loss: 7.8218\n",
      "Epoch 301/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8563 - val_loss: 7.8764\n",
      "Epoch 302/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8567 - val_loss: 7.9111\n",
      "Epoch 303/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8601 - val_loss: 7.8619\n",
      "Epoch 304/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8580 - val_loss: 7.7739\n",
      "Epoch 305/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8456 - val_loss: 7.8644\n",
      "Epoch 306/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8536 - val_loss: 7.9721\n",
      "Epoch 307/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8617 - val_loss: 7.8947\n",
      "Epoch 308/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8585 - val_loss: 7.7739\n",
      "Epoch 309/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8511 - val_loss: 7.8140\n",
      "Epoch 310/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8591 - val_loss: 7.9758\n",
      "Epoch 311/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8679 - val_loss: 7.7940\n",
      "Epoch 312/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8613 - val_loss: 7.8762\n",
      "Epoch 313/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8590 - val_loss: 7.8339\n",
      "Epoch 314/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8596 - val_loss: 7.8076\n",
      "Epoch 315/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8531 - val_loss: 7.8133\n",
      "Epoch 316/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8485 - val_loss: 7.8746\n",
      "Epoch 317/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8664 - val_loss: 7.8531\n",
      "Epoch 318/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8576 - val_loss: 7.7875\n",
      "Epoch 319/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8678 - val_loss: 7.8228\n",
      "Epoch 320/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8650 - val_loss: 7.8908\n",
      "Epoch 321/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8579 - val_loss: 7.8829\n",
      "Epoch 322/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8588 - val_loss: 7.8363\n",
      "Epoch 323/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8555 - val_loss: 7.9531\n",
      "Epoch 324/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8585 - val_loss: 7.9787\n",
      "Epoch 325/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8570 - val_loss: 7.9658\n",
      "Epoch 326/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8677 - val_loss: 7.9036\n",
      "Epoch 327/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8694 - val_loss: 7.8851\n",
      "Epoch 328/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8632 - val_loss: 7.9294\n",
      "Epoch 329/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8645 - val_loss: 7.8476\n",
      "Epoch 330/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8578 - val_loss: 7.9231\n",
      "Epoch 331/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8677 - val_loss: 7.8349\n",
      "Epoch 332/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8607 - val_loss: 7.8983\n",
      "Epoch 333/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8632 - val_loss: 7.9305\n",
      "Epoch 334/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8550 - val_loss: 7.8193\n",
      "Epoch 335/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8564 - val_loss: 7.8818\n",
      "Epoch 336/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8759 - val_loss: 7.8123\n",
      "Epoch 337/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8600 - val_loss: 7.8781\n",
      "Epoch 338/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8679 - val_loss: 7.7409\n",
      "Epoch 339/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8658 - val_loss: 7.8240\n",
      "Epoch 340/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8546 - val_loss: 7.8108\n",
      "Epoch 341/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8535 - val_loss: 7.8047\n",
      "Epoch 342/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8577 - val_loss: 7.8436\n",
      "Epoch 343/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8492 - val_loss: 7.8209\n",
      "Epoch 344/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8580 - val_loss: 7.8629\n",
      "Epoch 345/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8636 - val_loss: 7.9346\n",
      "Epoch 346/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8635 - val_loss: 7.7257\n",
      "Epoch 347/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8638 - val_loss: 7.9157\n",
      "Epoch 348/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8694 - val_loss: 7.8840\n",
      "Epoch 349/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8446 - val_loss: 7.7595\n",
      "Epoch 350/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8603 - val_loss: 7.8217\n",
      "Epoch 351/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8663 - val_loss: 7.9450\n",
      "Epoch 352/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8596 - val_loss: 7.9563\n",
      "Epoch 353/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8548 - val_loss: 7.8159\n",
      "Epoch 354/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8629 - val_loss: 7.7782\n",
      "Epoch 355/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8577 - val_loss: 7.9192\n",
      "Epoch 356/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8540 - val_loss: 7.7379\n",
      "Epoch 357/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8530 - val_loss: 7.8722\n",
      "Epoch 358/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8517 - val_loss: 7.7949\n",
      "Epoch 359/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8547 - val_loss: 7.7349\n",
      "Epoch 360/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8611 - val_loss: 7.8767\n",
      "Epoch 361/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8631 - val_loss: 7.7304\n",
      "Epoch 362/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8482 - val_loss: 7.8692\n",
      "Epoch 363/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8539 - val_loss: 7.7918\n",
      "Epoch 364/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8700 - val_loss: 7.8416\n",
      "Epoch 365/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8485 - val_loss: 7.9153\n",
      "Epoch 366/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8542 - val_loss: 7.9055\n",
      "Epoch 367/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8736 - val_loss: 7.8572\n",
      "Epoch 368/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8573 - val_loss: 7.8405\n",
      "Epoch 369/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8559 - val_loss: 7.7989\n",
      "Epoch 370/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8585 - val_loss: 7.9437\n",
      "Epoch 371/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8609 - val_loss: 7.7960\n",
      "Epoch 372/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8481 - val_loss: 8.0624\n",
      "Epoch 373/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8747 - val_loss: 7.9609\n",
      "Epoch 374/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8642 - val_loss: 7.7729\n",
      "Epoch 375/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8596 - val_loss: 7.8503\n",
      "Epoch 376/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8579 - val_loss: 7.8967\n",
      "Epoch 377/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8538 - val_loss: 7.7441\n",
      "Epoch 378/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8555 - val_loss: 7.8654\n",
      "Epoch 379/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8575 - val_loss: 7.8814\n",
      "Epoch 380/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8574 - val_loss: 7.7405\n",
      "Epoch 381/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8663 - val_loss: 7.8547\n",
      "Epoch 382/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8601 - val_loss: 7.8800\n",
      "Epoch 383/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8595 - val_loss: 7.8370\n",
      "Epoch 384/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8614 - val_loss: 7.7964\n",
      "Epoch 385/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8562 - val_loss: 7.8587\n",
      "Epoch 386/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8565 - val_loss: 7.8338\n",
      "Epoch 387/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8535 - val_loss: 7.8361\n",
      "Epoch 388/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8513 - val_loss: 7.8768\n",
      "Epoch 389/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8690 - val_loss: 7.8671\n",
      "Epoch 390/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8489 - val_loss: 7.9159\n",
      "Epoch 391/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8581 - val_loss: 7.8439\n",
      "Epoch 392/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8547 - val_loss: 7.9109\n",
      "Epoch 393/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8635 - val_loss: 7.8434\n",
      "Epoch 394/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8605 - val_loss: 7.9485\n",
      "Epoch 395/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8661 - val_loss: 7.8798\n",
      "Epoch 396/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8570 - val_loss: 7.7691\n",
      "Epoch 397/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8672 - val_loss: 7.8141\n",
      "Epoch 398/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8638 - val_loss: 7.8387\n",
      "Epoch 399/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8560 - val_loss: 7.8476\n",
      "Epoch 400/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8525 - val_loss: 7.9183\n",
      "Epoch 401/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8602 - val_loss: 7.8570\n",
      "Epoch 402/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8719 - val_loss: 7.8275\n",
      "Epoch 403/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8624 - val_loss: 7.9102\n",
      "Epoch 404/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8534 - val_loss: 7.8805\n",
      "Epoch 405/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8661 - val_loss: 7.9281\n",
      "Epoch 406/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8509 - val_loss: 7.8826\n",
      "Epoch 407/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8624 - val_loss: 7.7748\n",
      "Epoch 408/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8604 - val_loss: 7.7905\n",
      "Epoch 409/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8666 - val_loss: 7.9398\n",
      "Epoch 410/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8667 - val_loss: 7.8826\n",
      "Epoch 411/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8664 - val_loss: 7.6929\n",
      "Epoch 412/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8526 - val_loss: 7.8612\n",
      "Epoch 413/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8524 - val_loss: 7.9083\n",
      "Epoch 414/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8567 - val_loss: 7.8598\n",
      "Epoch 415/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8576 - val_loss: 7.9266\n",
      "Epoch 416/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8585 - val_loss: 7.8390\n",
      "Epoch 417/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8653 - val_loss: 7.8844\n",
      "Epoch 418/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8602 - val_loss: 7.7597\n",
      "Epoch 419/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8570 - val_loss: 7.7933\n",
      "Epoch 420/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8527 - val_loss: 7.9373\n",
      "Epoch 421/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8526 - val_loss: 7.9112\n",
      "Epoch 422/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8608 - val_loss: 7.7597\n",
      "Epoch 423/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8630 - val_loss: 7.8982\n",
      "Epoch 424/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8512 - val_loss: 7.7367\n",
      "Epoch 425/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8623 - val_loss: 7.8676\n",
      "Epoch 426/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8542 - val_loss: 7.7981\n",
      "Epoch 427/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8700 - val_loss: 7.8543\n",
      "Epoch 428/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8707 - val_loss: 7.8114\n",
      "Epoch 429/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8663 - val_loss: 8.0165\n",
      "Epoch 430/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8594 - val_loss: 7.9182\n",
      "Epoch 431/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8439 - val_loss: 7.9224\n",
      "Epoch 432/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8682 - val_loss: 7.9477\n",
      "Epoch 433/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8587 - val_loss: 7.7633\n",
      "Epoch 434/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8607 - val_loss: 7.9269\n",
      "Epoch 435/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8648 - val_loss: 7.8240\n",
      "Epoch 436/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8582 - val_loss: 7.7365\n",
      "Epoch 437/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8672 - val_loss: 7.8639\n",
      "Epoch 438/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8637 - val_loss: 7.8316\n",
      "Epoch 439/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8582 - val_loss: 7.8472\n",
      "Epoch 440/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8626 - val_loss: 7.7337\n",
      "Epoch 441/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8539 - val_loss: 7.8394\n",
      "Epoch 442/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8632 - val_loss: 7.8127\n",
      "Epoch 443/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8645 - val_loss: 7.9190\n",
      "Epoch 444/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8600 - val_loss: 7.7337\n",
      "Epoch 445/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8591 - val_loss: 7.8677\n",
      "Epoch 446/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8602 - val_loss: 7.8806\n",
      "Epoch 447/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8561 - val_loss: 7.9099\n",
      "Epoch 448/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8595 - val_loss: 7.8611\n",
      "Epoch 449/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8593 - val_loss: 7.8140\n",
      "Epoch 450/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8536 - val_loss: 7.9236\n",
      "Epoch 451/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8665 - val_loss: 7.8720\n",
      "Epoch 452/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8503 - val_loss: 7.8635\n",
      "Epoch 453/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8585 - val_loss: 7.8391\n",
      "Epoch 454/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8511 - val_loss: 7.8455\n",
      "Epoch 455/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8667 - val_loss: 7.8757\n",
      "Epoch 456/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8634 - val_loss: 7.8333\n",
      "Epoch 457/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8766 - val_loss: 7.8453\n",
      "Epoch 458/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8610 - val_loss: 7.8628\n",
      "Epoch 459/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8617 - val_loss: 7.8583\n",
      "Epoch 460/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8520 - val_loss: 7.8913\n",
      "Epoch 461/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8531 - val_loss: 7.8561\n",
      "Epoch 462/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8646 - val_loss: 7.7872\n",
      "Epoch 463/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8661 - val_loss: 7.8353\n",
      "Epoch 464/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8659 - val_loss: 7.8628\n",
      "Epoch 465/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8526 - val_loss: 7.8292\n",
      "Epoch 466/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8618 - val_loss: 7.8473\n",
      "Epoch 467/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8457 - val_loss: 7.9446\n",
      "Epoch 468/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8545 - val_loss: 7.8723\n",
      "Epoch 469/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8631 - val_loss: 7.8635\n",
      "Epoch 470/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8546 - val_loss: 7.9458\n",
      "Epoch 471/1000\n",
      "5000/5000 [==============================] - 182s 36ms/step - loss: 7.8574 - val_loss: 7.8351\n",
      "Epoch 472/1000\n",
      " 303/5000 [>.............................] - ETA: 2:54 - loss: 7.8707"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-dc743cec344b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen = prep.examples_generator(sents_train_clean)\n",
    "val = prep.examples_generator(sents_test_clean)\n",
    "# printer_callback = PrinterCallback(val, prep)\n",
    "cb = [\n",
    "    ModelCheckpoint('ecle_new.cnn', save_best_only=False),\n",
    "#     printer_callback\n",
    "]\n",
    "mlp.model.fit_generator(\n",
    "            gen,\n",
    "            steps_per_epoch=5000,\n",
    "            epochs=1000,\n",
    "            validation_data=val,\n",
    "            callbacks=cb,\n",
    "            validation_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import numpy\n",
    "# numpy.set_printoptions(threshold=sys.maxsize)\n",
    "'species' in prep.token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [layer.name for layer in model.trainable_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_opt_gradients(model, weight_names=[]):\n",
    "#     \"\"\"Return the gradient of every trainable weight in model\n",
    "\n",
    "#     Parameters\n",
    "#     -----------\n",
    "#     model : a keras model instance\n",
    "\n",
    "#     First, find all tensors which are trainable in the model. Surprisingly,\n",
    "#     `model.trainable_weights` will return tensors for which\n",
    "#     trainable=False has been set on their layer (last time I checked), hence the extra check.\n",
    "#     Next, get the gradients of the loss with respect to the weights.\n",
    "\n",
    "#     \"\"\"\n",
    "#     weights = [tensor for tensor in model.trainable_weights if tensor.name in weight_names]\n",
    "\n",
    "#     optimizer = model.optimizer\n",
    "\n",
    "#     return optimizer.get_gradients(model.total_loss, weights)\n",
    "\n",
    "# import keras.backend as K\n",
    "\n",
    "# tensornames = [\"context_embedding/embeddings:0\"]\n",
    "# input_tensors =  model.inputs + model.sample_weights + model.targets + [K.learning_phase()]\n",
    "# gradients = get_opt_gradients(model, tensornames)\n",
    "# get_gradients = K.function(inputs=input_tensors, outputs=gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = prep.examples_generator(sents_train)\n",
    "example = next(gen)\n",
    "example_x = example[0]\n",
    "example_y = example[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [example_x] + [np.ones_like(example_y)] + [example_y] + [0]\n",
    "grads = get_gradients(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, grad in enumerate(grads):\n",
    "    print(tensornames[i])\n",
    "    print(grad)\n",
    "    print(np.sum(grad))\n",
    "    print('*'*80)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[layer.name for layer in model.layers]\n",
    "print(np.mean(model.get_layer('context_embedding').get_weights()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('time_distributed_1').trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_modeler_inp, context_modeler_out = get_character_embedder(2*prep.max_example_len, char_emb_size, n_chars, 128, 'context')\n",
    "gmp_context_modeler = GlobalMaxPooling1D(name='context_gmp')(context_modeler_out)\n",
    "context_model_final = Model(context_modeler_inp, gmp_context_modeler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_ind, layer in enumerate(model.layers):\n",
    "    try:\n",
    "        new_layer = context_model_final.get_layer(layer.name)\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "        print(f\"layer transferred: {layer.name}\")\n",
    "        print(len(new_layer.get_weights()))\n",
    "        print(str(type(new_layer)))\n",
    "    except ValueError as v_error:\n",
    "        pass\n",
    "context_model_final.get_layer('embedding_3').set_weights(model.get_layer(\"embedding_1\").get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(new_layer, GlobalMaxPooling1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.get_layer('context_c1')\n",
    "weights = layer.get_weights()\n",
    "for w in weights:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1d = Conv1D(128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmf = context_model\n",
    "sentence_examples = [\"I took the train\",\n",
    "                     \"We took a long bus\",\n",
    "                     \"We took too long\",\n",
    "                     \"His name was Peregrin Took\",\n",
    "                     \"I take a trane\",\n",
    "                     \"My cat is grey\",\n",
    "                     \"Our dog is golden\",\n",
    "                     \"note that punctuation matters\",\n",
    "                     \"Note that punctuation matters\",\n",
    "                     \"Note that puncutation mattes\",\n",
    "                     \"Not punctual mates\",\n",
    "                     \"Washington crossed the river\",\n",
    "                     \"Washington won the match\",\n",
    "                     \"Cortez crossed the river\"]\n",
    "# sentence_examples = [\"I took the train\", \"We got on the bus\", \"I trained on it\"]\n",
    "example_contexts = np.array([prep.string_to_array(s, prep.max_example_len) for s in sentence_examples])\n",
    "# for nn, c in enumerate(example_contexts):\n",
    "#     aslist = c.tolist()\n",
    "#     aslist[aslist.index(0)] = 2\n",
    "#     example_contexts[nn] = np.array(aslist)\n",
    "print(example_contexts)\n",
    "preds = cmf.predict(example_contexts)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim = cosine_similarity(preds)\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=2)\n",
    "df = pd.DataFrame([[str(float(j)) for j in i] for i in sim.tolist()], columns=sentence_examples, index=sentence_examples)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in context_model.layers:\n",
    "    if len(layer.get_weights()) > 0:\n",
    "        weights = layer.get_weights()\n",
    "        for weight in weights:\n",
    "            print(layer.name, np.max(weight), np.mean(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = model.get_layer('context_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = l.get_weights()\n",
    "w[0][0] = np.zeros_like(w[0][0])\n",
    "l.set_weights(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_layer('context_embedding').get_weights()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ['a b c d e f g h s', 'a e b f g h', 'j c d s a k l m', 'j o o o [ p q r s', 't u v w x y z']\n",
    "prep = ecle.Preprocessor()\n",
    "prep.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = prep.examples_generator(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev(prep, a):\n",
    "    return ''.join([prep.char_rev[int(c)] for c in a])\n",
    "x, y = next(gen)\n",
    "rev(prep, x[0]), y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save(p, path):\n",
    "    \"\"\"\n",
    "    Write a Preprocessor object to a .JSON config\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"char_rev\": p.char_rev,\n",
    "        \"char_dict\": p.char_dict,\n",
    "        \"max_example_len\": p.max_example_len,\n",
    "    }\n",
    "    with open(os.path.join(path, \"ecle_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "save(prep, \"models/prod/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "context_copy = load_model('models/prod/ecle.cnn', custom_objects={'clipped_bce': clipped_bce})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_input = context_copy._layers.pop(0)\n",
    "new_input = Input((256,))\n",
    "context_copy._layers[0].inbound_nodes = []\n",
    "context_copy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out = context_copy(new_input)\n",
    "new_model = Model(new_input, context_copy.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = context_copy.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecle\n",
    "prep = ecle.Preprocessor(num_stopwords=0)\n",
    "ex = \"f! this is a very long string and I want to see what will happen when it actually gets parsed I guess it will depend on how things work eh?\"\n",
    "print(len(ex))\n",
    "prep.fit([\"hi there friend\", ex])\n",
    "print(prep.token_dict)\n",
    "print(prep.char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = \"hi there, friend\"\n",
    "x, y = prep.string_to_example(ex)\n",
    "print(x)\n",
    "print(y.max(), y.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = prep.examples_generator([\"hi there friend!\", \"hi you, you are my friend\"])\n",
    "g = next(gen)\n",
    "for exp in g[0]:\n",
    "    print(''.join(prep.char_rev[i] for i in exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The band was ???? in 2005.\n",
      "***: established\n",
      "('family', 0.008890179)\n",
      "('species', 0.008009727)\n",
      "('born', 0.00791195)\n",
      "('united', 0.005558612)\n",
      "('this', 0.005556354)\n",
      "('that', 0.0053356793)\n",
      "('first', 0.005127159)\n",
      "('also', 0.0050174645)\n",
      "('#', 0.0047546485)\n",
      "('his', 0.0046944735)\n"
     ]
    }
   ],
   "source": [
    "ev = next(val)\n",
    "ex = ev[0][0]\n",
    "ans = ev[1][0]\n",
    "\n",
    "def check_example(ex, ans, model, prep, n=10):\n",
    "    print(''.join([prep.char_rev[i] for i in ex]))\n",
    "    print(f'***: {prep.token_rev[np.argmax(ans)]}')\n",
    "    pred = model.predict(ex.reshape(1, -1))[0]\n",
    "    top_token_inds = np.argpartition(pred, -n)[-n:]\n",
    "    top_tokens = [(prep.token_rev[j], pred[j]) for j in top_token_inds]\n",
    "    top_sort = sorted(top_tokens, key=lambda x: x[1], reverse=True)\n",
    "    print('\\n'.join([str(t) for t in top_sort]))\n",
    "#     return pred\n",
    "p = check_example(ex, ans, mlp.model, prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(ans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('\\d', '#', '1,133rd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Model(ecle_base.embedder.input, gmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "# example_strs = [\n",
    "#     \"Born in Morristown, he grew up in New Jersey.\",\n",
    "#     \"Born in New Jersey, he grew up in Morristown\",\n",
    "#     \"New Jersey grows a lot of corn.\",\n",
    "#     \"New Jersey grows a lot of tomatoes.\",\n",
    "#     \"New jerseys cost a lot of money\"\n",
    "#     \"New Jersey corn is sweeter\",\n",
    "#     \"Bor in Motown, he growed up in New York\"\n",
    "# ]\n",
    "example_strs = [\n",
    "    # Smartphones\n",
    "    \"I like my phone\",\n",
    "    \"My phone is not good\",\n",
    "    \"Your cellphone looks great\",\n",
    "\n",
    "    # Weather\n",
    "    \"Will it snow tomorrow\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "\n",
    "    # Food and health\n",
    "    \"An apple a day keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "\n",
    "    # Asking about age\n",
    "    \"How old are you\",\n",
    "    \"what is your age\",\n",
    "]\n",
    "examples = np.array([prep.string_to_array(i, prep.max_example_len) for i in example_strs])\n",
    "preds = embedder.predict(examples)\n",
    "print(preds.shape)\n",
    "cs = cosine_similarity(preds)\n",
    "df = pd.DataFrame(cs, columns=example_strs, index=example_strs)\n",
    "with pd.option_context('max_columns', None):\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ecle_inp (InputLayer)           (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ecle_embedding (Embedding)      (None, 128, 32)      3168        ecle_inp[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c1 (Conv1D)                (None, 128, 256)     24832       ecle_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 256)     0           ecle_c1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c2 (Conv1D)                (None, 128, 256)     196864      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c3 (Conv1D)                (None, 128, 256)     196864      ecle_c2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ecle_res_conn (Add)             (None, 128, 256)     0           ecle_c1[0][0]                    \n",
      "                                                                 ecle_c3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 256)     0           ecle_res_conn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ecle_c4 (Conv1D)                (None, 128, 1024)    787456      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "words (Conv1D)                  (None, 128, 128)     393344      ecle_c4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128, 128)     0           words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_5 (Permute)             (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_7 (Permute)             (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_0 (Dense)                   (None, 128, 128)     16512       permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_1 (Dense)                   (None, 128, 128)     16512       permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_2 (Dense)                   (None, 128, 128)     16512       permute_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_3 (Dense)                   (None, 128, 128)     16512       permute_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 128, 128)     0           att_0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 128, 128)     0           att_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_6 (Permute)             (None, 128, 128)     0           att_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_8 (Permute)             (None, 128, 128)     0           att_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "                                                                 permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "                                                                 permute_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 128, 128)     0           dropout_3[0][0]                  \n",
      "                                                                 permute_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 512)     0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "                                                                 multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 512)     786944      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10000)        5130000     global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 7,585,520\n",
      "Trainable params: 7,585,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "emb_mat = model.get_layer(\"ecle_embedding\").get_weights()[0]\n",
    "tsne = PCA(n_components=2)\n",
    "x = tsne.fit_transform(emb_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[:, 0], x[:, 1], s=0.01)\n",
    "\n",
    "val_min = np.min(list(prep.char_dict.values()))\n",
    "\n",
    "for i, txt in cr.items():\n",
    "    i2 = i - val_min\n",
    "    plt.annotate(txt, (x[i2, 0], x[i2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 4,\n",
       " ' ': 5,\n",
       " '!': 6,\n",
       " '\"': 7,\n",
       " '#': 8,\n",
       " '$': 9,\n",
       " '%': 10,\n",
       " '&': 11,\n",
       " \"'\": 12,\n",
       " '(': 13,\n",
       " ')': 14,\n",
       " '*': 15,\n",
       " '+': 16,\n",
       " ',': 17,\n",
       " '-': 18,\n",
       " '.': 19,\n",
       " '/': 20,\n",
       " '0': 21,\n",
       " '1': 22,\n",
       " '2': 23,\n",
       " '3': 24,\n",
       " '4': 25,\n",
       " '5': 26,\n",
       " '6': 27,\n",
       " '7': 28,\n",
       " '8': 29,\n",
       " '9': 30,\n",
       " ':': 31,\n",
       " ';': 32,\n",
       " '<': 33,\n",
       " '=': 34,\n",
       " '>': 35,\n",
       " '?': 36,\n",
       " '@': 37,\n",
       " 'A': 38,\n",
       " 'B': 39,\n",
       " 'C': 40,\n",
       " 'D': 41,\n",
       " 'E': 42,\n",
       " 'F': 43,\n",
       " 'G': 44,\n",
       " 'H': 45,\n",
       " 'I': 46,\n",
       " 'J': 47,\n",
       " 'K': 48,\n",
       " 'L': 49,\n",
       " 'M': 50,\n",
       " 'N': 51,\n",
       " 'O': 52,\n",
       " 'P': 53,\n",
       " 'Q': 54,\n",
       " 'R': 55,\n",
       " 'S': 56,\n",
       " 'T': 57,\n",
       " 'U': 58,\n",
       " 'V': 59,\n",
       " 'W': 60,\n",
       " 'X': 61,\n",
       " 'Y': 62,\n",
       " 'Z': 63,\n",
       " '[': 64,\n",
       " '\\\\': 65,\n",
       " ']': 66,\n",
       " '^': 67,\n",
       " '_': 68,\n",
       " '`': 69,\n",
       " 'a': 70,\n",
       " 'b': 71,\n",
       " 'c': 72,\n",
       " 'd': 73,\n",
       " 'e': 74,\n",
       " 'f': 75,\n",
       " 'g': 76,\n",
       " 'h': 77,\n",
       " 'i': 78,\n",
       " 'j': 79,\n",
       " 'k': 80,\n",
       " 'l': 81,\n",
       " 'm': 82,\n",
       " 'n': 83,\n",
       " 'o': 84,\n",
       " 'p': 85,\n",
       " 'q': 86,\n",
       " 'r': 87,\n",
       " 's': 88,\n",
       " 't': 89,\n",
       " 'u': 90,\n",
       " 'v': 91,\n",
       " 'w': 92,\n",
       " 'x': 93,\n",
       " 'y': 94,\n",
       " 'z': 95,\n",
       " '{': 96,\n",
       " '|': 97,\n",
       " '}': 98,\n",
       " '~': 99}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_prerained = Model(ecle_base.embedder.input, final_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_prerained.save(\"models/prod/lm_pre.cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.save(\"models/prod/lm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmf/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/engine/network.py:180: UserWarning:\n",
      "\n",
      "Model inputs must come from `keras.layers.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to your model was not an Input tensor, it was generated by layer dropout_3.\n",
      "Note that input tensors are instantiated via `tensor = keras.layers.Input(shape)`.\n",
      "The tensor that caused the issue was: dropout_3/cond/Merge:0\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"ecle_inp:0\", shape=(?, 128), dtype=float32) at layer \"ecle_inp\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-70d29eebc2ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdifferent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 231\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recipe/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                                          \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                          \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"ecle_inp:0\", shape=(?, 128), dtype=float32) at layer \"ecle_inp\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "different = Model(new_conv, final_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
